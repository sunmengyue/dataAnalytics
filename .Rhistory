#Assign proper column names to the dataset
colnames(myData) <- c("visitNum", "transNum", "opSys", "gender", "revenue")
#replace the string NA with actual NA
myData[myData == "NA"] <- NA
#transform transaction number from string into numeric
myData$transNum <- as.numeric(myData$transNum)
typeof(myData$transNum)
#a. total number of cases after excluding missing cases
nrow(na.omit(myData))
#b. mean number of visits
mean(myData$visitNum, na.rm = TRUE)
#c. median revenue
median(myData$revenue, na.rm = TRUE)
#d. maximum and minimum number of transactions
#maximum transaction number:
max(myData$transNum, na.rm = TRUE)
#minimum transaction number:
min(myData$transNum, na.rm = TRUE)
#most commonly used operating system
table(myData$opSys)
#from the frequency table we know that the most commonly used operating system is Android
# a scatterplot of number of visits (x-axis) versus revenue (y-axis)
library(ggplot2)
ggplot(na.omit(myData), aes(x = visitNum, y = revenue)) + geom_jitter()
#identify which column(s) has/have missing data
colnames(myData)[colSums(is.na(myData)) > 0]
median(myData$transNum, na.rm = TRUE)
median(myData$transNum, na.rm = TRUE)
myData$transNum[is.na(myData$transNum)] <- 1
table(is.na(myData$opSys))
mode(myData$gender)
table(myData$gender)
myData$gender[is.na(myData$gender)] <- "Male"
table(is.na(myData$gender))
table(myData$gender)
?seq()
odd <- seq(from = 1, to = 22799, by = 2)
even <- seq(from = 2, to = 22800, by = 2)
head(odd)
head(even, 10)
training <- slice(myData, odd)
View(training)
validation <- slice(myData, even)
View(validation)
mean(training$revenue)
mean(validation$revenue)
library(modelr)
trainVal <- resample_partition(myData, c(train = 0.6, valid = 0.4))
trainVal
View(trainVal)
trainVal
library(tidyr)
library(readxl)
myData <- read_excel(
"/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/week 2/homework1/customertxndata.xlsx"
)
head(myData)
str(myData)
#Assign proper column names to the dataset
colnames(myData) <- c("visitNum", "transNum", "opSys", "gender", "revenue")
#replace the string NA with actual NA
myData[myData == "NA"] <- NA
#transform transaction number from string into numeric
myData$transNum <- as.numeric(myData$transNum)
typeof(myData$transNum)
#a. total number of cases after excluding missing cases
nrow(na.omit(myData))
#b. mean number of visits
mean(myData$visitNum, na.rm = TRUE)
#c. median revenue
median(myData$revenue, na.rm = TRUE)
#d. maximum and minimum number of transactions
#maximum transaction number:
max(myData$transNum, na.rm = TRUE)
#minimum transaction number:
min(myData$transNum, na.rm = TRUE)
#most commonly used operating system
table(myData$opSys)
#from the frequency table we know that the most commonly used operating system is Android
# a scatterplot of number of visits (x-axis) versus revenue (y-axis)
library(ggplot2)
ggplot(na.omit(myData), aes(x = visitNum, y = revenue)) + geom_jitter()
#identify which column(s) has/have missing data
colnames(myData)[colSums(is.na(myData)) > 0]
median(myData$transNum, na.rm = TRUE)
myData$transNum[is.na(myData$transNum)] <- 1
#check if all the NAs are imputed
table(is.na(myData$opSys))
#get the mode of gender
table(myData$gender)
#We can see most of the customers are male, so we will impute the missing gender with mald
myData$gender[is.na(myData$gender)] <- "Male"
#check if all the NAs are imputed
table(is.na(myData$gender))
#We see there is no na in the gender column now
table(myData$gender)
#also, the number of male custormers increased after imputation
odd <- seq(from = 1, to = 22799, by = 2)
even <- seq(from = 2, to = 22800, by = 2)
#Assign odd-numbered rows into training dataset (using the dplyr package inside tidyverse)
training <- slice(myData, odd)
#Assign even-numbered rows into validation set
validation <- slice(myData, even)
#Calculate the mean revenue for the training data set
mean(training$revenue)
#Calculate the mean revenue for the validation data set
mean(validation$revenue)
# modelr package is a part of the tidyverse package, but not loaded when the tidyverse package is loaded
library(modelr)
#use resample_partition to create training and validation datasets
trainVal <- resample_partition(myData, c(train = 0.6, valid = 0.4))
trainVal
data("LakeHuron")
str("LakeHuron")
head("LakeHuron")
head(LakeHuron)
View(LakeHuron)
year <- 1875 : 1972
df <- data.frame(year, LakeHuron)
View(df)
nrow(df)
tail(df)
last4 <- df[c(n, n-1), 2]
n <- nrow(df)
last4 <- df[c(n, n-1), 2]
last4
last4 <- df[n : (n - 3)]
last4 <- df[n : (n - 3), 2]
last4
last4 <- df[n : (n - 3)]
last4 <- df[n : (n - 3), 2]
last4
mean(last4)
last4
w <- c(3, 2, 1, 1)
tail(df)
sw <- w * last4
sw
F <- sum(sw)/sum(w)
F
df$Ft = 0
df$E = 0
df
head(df)
df$Ft[1] <- df[1, 2]
head(df)
df$Ft[2] <- df$Ft[1] + a * df$E[1]
df$Ft[2] <- df$Ft[1] + 0.3 * df$E[1]
df
head(df)
df$E[2] <- df[2, 2] - df$Ft[2]
head(df)
for (i in 2:nrow(df)) {
df$Ft[i] <- df$Ft[i - 1] + 0.3 * E[i - 1]
df$E[i] <- df[i, 2] - df$Ft[i]
}
for (i in 2:nrow(df)) {
df$Ft[i] <- df$Ft[i - 1] + 0.3 * E[i - 1]
df$E[i] <- df[i, 2] - df$Ft[i]
}
df$Ft[i] <- df$Ft[i - 1] + 0.3 * df$E[i - 1]
head(df)
for (i in 2:nrow(df)) {
df$Ft[i] <- df$Ft[i - 1] + 0.3 * df$E[i - 1]
df$E[i] <- df[i, 2] - df$Ft[i]
}
head(df)
tail(df)
n <- nrow(df)
Forcast <- df$Ft[n] + 0.3 * df$E[n]
Forcast
8 / 75
14/60
library(readxl)
meningitis <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/Biostatistics/homework2/meningitis.xlsx")
View(meningitis)
meningitis <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/Biostatistics/homework2/meningitis.xlsx")
View(meningitis)
mean(meningitis$miningitis)
median(meningitis$miningitis)
order(meningitis$miningitis)
seq(meningitis$miningitis)
sort(meningitis$miningitis)
mode(meningitis$miningitis)
summary(meningitis)
mode(meningitis)
table(meningitis$miningitis)
96 - 0.1
var(meningitis$miningitis)
sd(meningitis$miningitis)
28.22601^2
28.23 / 26.45
13 * 0.75
13 * 0.25
IQR(meningitis$miningitis)
32 + 1.5*27
1.5 * 27
1/6
3 - 2* (1/6)
180.62 - 288
1 - 1/36
35/36
3 - 1/3
3 + 1/3
ldl <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/Biostatistics/homework2/LDL.xlsx")
View(ldl)
ldl <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/Biostatistics/homework2/LDL.xlsx")
View(ldl)
library(tidyverse)
breaks <- seq(from = min(ldl$ldl), to = max(ldl$ldl), by = 50)
pop <- cut(ldl$ldl, breaks = breaks, right = FALSE, include.lowest = TRUE)
myLdl <- cut(ldl$ldl, breaks = breaks, right = FALSE, include.lowest = TRUE)
cbind(table(myLdl))
breaks <- seq(from = 0, to = max(ldl$ldl), by = 50)
myLdl <- cut(ldl$ldl, breaks = breaks, right = FALSE, include.lowest = TRUE)
cbind(table(myLdl))
0.123^5
0.123^2
0.877^2
install.packates("rmarkdown")
install.packages("rmarkdown")
install.packages("Rcmdr")
library(ggplot2)
ggplot(cars) + geom_point(aes(x = speed, y = dist))
cars %>% ggplot(aes(x = spped, y = dist)) + geom_point()
library(tidyverse)
cars %>% ggplot(aes(x = spped, y = dist)) + geom_point()
cars %>% ggplot(aes(x = speed, y = dist)) + geom_point()
iris %>% qplot(Petal.Width, Petal.Length, color = Species, data = .)
iris %>% ggplot(aes(x = Petal.Width, y = Petal.Length, color = Species)) + geom_point()
iris %>% ggplot() + geom_point(aes(x = Petal.Width, y = Petal.Length, color = Species))
#If you want to hard write color or any other graphics parameter, you just need to move the
#parameter assignment outside the aes() call
iris %>% ggplot + geom_point(aes(x = Petal.Width, y = Petal.Length), color = "red")
#qplot plot histogram and a density plot
cars %>% qplot(speed, data = ., bins = 10)
cars %>% qplot(speed, data = ., geom = "density")
cars %>% ggplot + geom_histogram(aes(x = speed), bins = 10)
cars %>% ggplot + geom_density(aes(x = speed))
cars %>% ggplot(aes(x = speed, y = ..count..)) + geom_histogram(bins = 10) + geom_density()
cars %>% ggplot(aes(x = speed, y = dist)) +
geom_point() + geom_smooth(method = "lm")
cars %>% ggplot(aes(x = speed, y = dist)) +
geom_point() + geom_smooth(method = "lm")
cars %>% ggplot(aes(x = speed, y = dist)) +
geom_point() + geom_smooth()
#Use more than one geometric
longley %>% ggplot(aes(x = Year)) +
geom_line(aes(y = Unemployed)) +
geom_line(aes(y = Armed.Forces), color = "blue")
longley %>% ggplot(aes(x = Year)) +
geom_point(aes(y = Unemployed)) +
geom_point(aes(y = Armed.Forces), color = "blue") +
geom_line(aes(y = Unemployed)) +
geom_line(aes(y = Armed.Forces), color = "blue")
View(longley)
longley %>% gather(key, value, Unemployed, Armed.Forces)
longley %>% gather(key, value, Unemployed, Armed.Forces) %>%
ggplot(aes(x = Year, y = value, color = key)) + geom_line()
longley %>% gather(key, value, Unemployed, Armed.Forces) %>%
ggplot(aes(x = Year, y = value, color = key)) + geom_line() +
facet_grid(key ~ .)
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
pcancer <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/Assignment3/prostate_cancer.xlsx")
View(pcancer)
?read_excel()
pcancer <- read_excel("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/Assignment3/prostate_cancer.xlsx", col_names = FALSE)
View(pcancer)
setwd("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/Assignment3")
prc <- read.csv("prostate_cancer(1).csv")
prc <- read.csv("prostate_cancer (1).csv")
prc <- read.csv("prostate_cancer (1).csv", strAsFactors = FALSE)
prc <- read.csv("prostate_cancer (1).csv", stringAsFactors = FALSE)
prc <- read.csv("prostate_cancer (1).csv", stringsAsFactors = FALSE)
str(prc)
View(prc)
prc <- prc[-1]
head(prc)
head(prc)
table(prc$diagnosis_result)
prc$diagnosis <- factor(prc$diagnosis_result, levles = c("B", "M"), lables = c("Benign", "Malignant"))
prc$diagnosis <- factor(prc$diagnosis_result, levels = c("B", "M"), lables = c("Benign", "Malignant"))
prc$diagnosis <- factor(prc$diagnosis_result, levels = c("B", "M"), labels = c("Benign", "Malignant"))
View(prx)
View(prc)
table(prc$diagnosis)
round(prop.table(table(prc$diagnosis)) * 100, digits = 1)
normalize <- function(x) {
return( (x - min(x)) / (max(x) - min(z)) )
}
prc_n <- as.data.frame(lapply(prc[2:9], normalize))
return( (x - min(x)) / (max(x) - min(x)) )
normalize <- function(x) {
return( (x - min(x)) / (max(x) - min(x)) )
}
prc_n <- as.data.frame(lapply(prc[2:9], normalize))
summary(prc_n$radius)
prc_train <- prc_n[1:65, ]
prc_test <- prc_n[66:100, ]
prc_train <- prc_n[1:65, ]
prc_test <- prc_n[66:100, ]
View(prc_n)
prc_train_lables <- prc[1:65, 1]
prc_test_lables <- prc[66:100, 1]
library(class)
prc_train_labels <- prc[1:65, 1]
prc_test_labels <- prc[66:100, 1]
prc_test_pred <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 10)
prc_test_pred
install.packages("gmodels")
library(gmodels)
CrossTable(x = prc_test_labels, y = prc_test_pred, prop.chisq = FALSE)
prc_test_pred8 <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 8)
CrossTable(x = prc_test_labels, y = prc_test_pred, prop.chisq = FALSE)
prc_test_pred8 <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 8)
CrossTable(x = prc_test_labels, y = prc_test_pred8, prop.chisq = FALSE)
prc_test_pred9 <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 9)
CrossTable(x = prc_test_labels, y = prc_test_pred9, prop.chisq = FALSE)
prc_test_pred11 <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 11)
CrossTable(x = prc_test_labels, y = prc_test_pred11, prop.chisq = FALSE)
prc_test_pred12 <- knn(train = prc_train, test = prc_test, cl = prc_train_labels, k = 12)
CrossTable(x = prc_test_labels, y = prc_test_pred12, prop.chisq = FALSE)
# The Caret Tutorial
library(C5O)
install.packages("C5O")
# The Caret Tutorial
library(C50)
install.packages("C50")
data(churn)
data()
data(package = "C50")
str(churnTrain)
data(churnTrain)
data(churn)
# The Caret Tutorial
library(C5.0)
# The Caret Tutorial
library(C50)
data(churn)
str(churTrain)
str(churnTrain)
View(churnTrain)
table(churnTest$churn)
unique(churnTest$churn)
predictors <- names(churnTrain)[names(churnTrain) != "churn"]
View(predictors)
str(predictors)
library(caret)
install.packages("caret")
set.seed(1)
library(caret)
inTrainingSet <- createDataPartition(allData$churn, p = 0.75, list = FALSE)
#A simple, stratified random split
allData <- data.frame(cbind(churnTrain, churnTest))
#A simple, stratified random split
allData <- data.frame(rbind(churnTrain, churnTest))
str(allData)
inTrainingSet <- createDataPartition(allData$churn, p = 0.75, list = FALSE)
inTrainingSet <- createDataPartition(allData$churn, p = 0.75, list = FALSE)
churn_Train <- allData[inTrainingSet, ]
set.seed(1)
inTrainingSet <- createDataPartition(allData$churn, p = 0.75, list = FALSE)
churn_Test <- allData[-inTriningSet, ]
churn_Test <- allData[-inTrainingSet, ]
View(churn_Train)
#Pre-processing Methods
numerics <- c("account_length", "total_day_calls", "total_night_calls")
## Determine means and sds
procValues <- preProcess(churnTrain[, numerics], method = c("center", "scale", "YeoJohnson"))
trainScaled <- predict(procValues, churnTrain[, numerics])
testScaled <- predict(procValues, churnTest[, numerics])
str(churnTrain)
procValues
install.packages("gbm")
forGBM <- churnTrain
forGBM$churn <- ifelse(forGBM$churn == "yes", 1, 0)
gbmFit <- gbm(formula = churn ~ .,
distribution = "bernoulli",
data = forGBM,
n.trees = 2000,
interaction.depth = 7,
shrinkage = 0.01,
verbose = FALSE)
#Boosted Trees
library(gbm)
gbmFit <- gbm(formula = churn ~ .,
distribution = "bernoulli",
data = forGBM,
n.trees = 2000,
interaction.depth = 7,
shrinkage = 0.01,
verbose = FALSE)
gbmFit <- gbm(formula = churn ~ .,
distribution = "bernoulli",
data = forGBM,
n.trees = 2000,
interaction.depth = 7,
shrinkage = 0.01,
verbose = FALSE)
gbmFit
gbmTune <- train(x = churnTrain[, predictors],
y = churnTrain$churn,
method = "gbm")
gbmTunez <- train(churn ~ ., data = churnTrain, method = gbm)
library(caret)
dataurl <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
setwd("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/data analytics")
download.file(url = dataurl, destfile = "wine.data")
wine_df <- read.csv("wine.data", header = FALSE)
Vie(wine_df)
View(wine_df)
str(wine_df)
#Data slicing
set.seed(3033)
training <- wine_df[intrain, ]
intrain <- createDataPartition(y = wine_df$V1, p= 0.7, list = FALSE) #target variable is at V1, not returning a list
training <- wine_df[intrain, ]
testing <- wine_df[-intrain,]
View(training)
View(intrain)
dim(training)
dim(testing)
anNA(wine_dif)
anyNA(wine_dif)
wine_df
anyNA(wine_dif)
str(wine_df)
anyNA(wine_df)
summary(wine_df)
training$V1 = factor(training$V1)
#Training the Knn model
#trainControl() controls the computational nuances of the train() method.
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(3333)
knn_fit <- train(V1 ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
#Test set prediction
> test_pred <- predict(knn_fit, newdata = testing)
#Test set prediction
test_pred <- predict(knn_fit, newdata = testing)
test_pred
#Evaluation with Confusion Matrix
confusionMatrix(test_pred, testing$V1)
testing <- wine_df[-intrain, ]
#Evaluation with Confusion Matrix
confusionMatrix(test_pred, testing$V1)
View(testing)
tesging$V1 = factor(testing$V1)
testing$V1 = factor(testing$V1)
#Evaluation with Confusion Matrix
confusionMatrix(test_pred, testing$V1)
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/Assignment3") #set the working directory
prc <- read.csv("prostate_cancer (1).csv", stringsAsFactors = FALSE) #import the data
str(prc) #check the data structure
set.seed(1221)
library(caret)
View(prc)
prc <- prc[-1]
head(prc)
table(prc$diagnosis_result) #check how many cases under each value of the diagnosis_result column
prc$diagnosis <- factor(prc$diagnosis_result, levels = c("B", "M"), labels = c("Benign", "Malignant"))
round(prop.table(table(prc$diagnosis)) * 100, digits = 1) #result in the percentage form will round to 1 decimal point
View(prc)
intrain <- createDataPartition(y = prc$diagnosis_result, p= 0.7, list = FALSE) #target variable is at V1, not returning a list
training <- prc[intrain, ]
testing <- prc[-intrain, ]
dim(training)
str(training)
str(training)
training <- prc[intrain, 2:9]
testing <- prc[-intrain, 2:9]
str(training)
str(testing)
str(prc$diagnosis_result)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(1234)
knn_fit <- train(V1 ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit <- train(diagnosis ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
training <- prc[intrain]
testing <- prc[-intrain]
knn_fit <- train(diagnosis ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
str(training)
str(training)
training <- prc[intrain, ]
testing <- prc[-intrain, ]
str(training)
str(testing)
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(1234)
knn_fit <- train(diagnosis ~., data = training, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
knn_fit
test_pred <- predict(knn_fit, newdata = testing)
test_pred
confusionMatrix(test_pred, testing$V1)
confusionMatrix(test_pred, testing$diagnosis)
