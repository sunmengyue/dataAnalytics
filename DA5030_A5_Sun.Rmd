---
title: "Assignment 5"
author: "Mengyue Sun"
date: "10/27/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 1
##Step 1 & Step 2: Collecting, Exploring and Preparing the Data
```{r }
credit <- read.csv("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/week 7/Assignment 5/credit .csv")
str(credit)
#replace "no" with "1" and "yes" with "2" in the code that for matrix_dimensions
credit$default[credit$default == 1] <- 'No'
credit$default[credit$default == 2] <- 'Yes'
credit$default <- factor(credit$default)
str(credit$default)
table(credit$checking_balance)
table(credit$savings_balance)
summary(credit$months_loan_duration)
summary(credit$amount)
table(credit$default)

```

###Data Preparation - Creating Random Training and Test Datasets
```{r }
#generize random number for the training dataset
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)

credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]

#check the randomization
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```
##Step 3. Training a Model on Data
```{r }
library(C50)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
summary(credit_model)
```
Translation of the first 4 lines of the output:
1. If the checking account balance is greater than 200 DM or is unknown, then we classify it as "not likely to default."

2. Otherwise, if the checking account balance is less than zero DM or between one and 200 DM:
3. And other_debtors are guarantor:
4. And the month loan duration is greater than 36, then we classify it as "likely to default."
On the first line, 412/50 indicates that among the 412 examples classified as not likely to default, 50 were classified incorrectly.

Translation of the confusion matrix:
The model incorrectly classified 135 of the 900 training instances for an error rate of 15 percent. Among the 135 incorrectly classfied cases, 44 no values were incorrectly classified as yes (false positives), while 91 yes values were misclassified as no (false negatives).

##Step 4. Evaluateing the Model Performance
```{r }
credit_pred <- predict(credit_model, credit_test)
library(gmodels)
CrossTable(credit_test$default, credit_pred, 
           prop.chisq = FALSE, 
           prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted   default'))
```
Among the 100 test loan application records, the model correctly predicted that 60 did not default and 14 did default, resulting in an accuracy of (60 + 14) /100 = 74%  and an error rate of (7 + 14) / 100 = 21%. Expectedly, the performance on the unseen tested data is worse than that on the training data. The model also only correctly predicted 14 of the 33 actual loan defaults in the test data. 

##Step 5. Improving Model Performance
```{r }
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
summary(credit_boost10) #The tree size shrunk
```
We can tell that the new classfier made 3 + 26 = 29 missclassfications out of 900 training samples for an error rate of 3.2%, which is quite an improvement compared to the previous 15%. 
```{r }
credit_boost_pred10 <- predict(credit_boost10, credit_test)

CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```
Now we see that the total accuracy rate is (60 + 16) / 100 = 76% versus the total error rate of (7 + 17) / 100 = 24%. The model is still not doing well as the 3% increasement in error rate. 

###Making mistakes more costlier than others
```{r}
#construct the cost matrix
matrix_dimensions <- list(c("No", "Yes"), c("No", "Yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
```
The values are supplied in specific order
Predicted no, actual no
Predicted yes, actual no
Predicted no, actual yes
Predicted yes, actual yes
```{r}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
```
There is no cost when the algorithm classifies a no or yes correctly, yet false negative has a cost of 4 versus a false positive's cost of 1. We will see how this impact the classification:
```{r}
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, 
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```
The error rate boosts significantly in this version, which is (34 + 7) / 100 = 41% 

#Problem 2.
##Step 1 & 2 Collecting, Exploring, and Preparing the Data
```{r }
mushrooms <- read.csv("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/week 7/Assignment 5/mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
#drop the veil_type variable
mushrooms$veil_type <- NULL
table(mushrooms$type)
#4208 of the mushrooms are edible, while 3916 are poisonous.
```

##Step 3. Training the Model on the Data
```{r }
library(RWeka)
mushroom_1R <- OneR(type ~ ., data = mushrooms)#consider all possible features
mushroom_1R
```
The odor feature was selected for rule generation. If the mushroom smells fishy, foul, musty, pungent, spicy, or creosote-like, the mushroom tend to be poisonous. On the other hand, if the mushroom smells like almond and anise, or those with no smell are predicted to be edible.

##Step 4. Evaluating the Model Performance
```{r}
summary(mushroom_1R)
```
The classifier did not classify any edible mushrooms as poisonous, it classify 120 poisonous mushrooms as edible.

##Step 5. Improving Model Performance
```{r}
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
```
The first 3 line can be translated as:
If the odor is foul, then the mushroom type is poisonous.
If the gill size is narrow and the gill color is buff, then the mushroom type is poisonous.
If the gill size is narrow and the odor is pungent, then the mushroom type is poisonous
Mushrooms not covered by the 9 rules are edible.
```{r }
summary(mushroom_JRip)
```
This time we see no mushrooms are misclassified.

#Problem 3
