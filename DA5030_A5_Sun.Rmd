---
title: "Assignment 5"
author: "Mengyue Sun"
date: "10/27/2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Problem 1
##Step 1 & Step 2: Collecting, Exploring and Preparing the Data
```{r }
credit <- read.csv("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/week 7/Assignment 5/credit .csv")
str(credit)
#replace "no" with "1" and "yes" with "2" in the code that for matrix_dimensions
credit$default[credit$default == 1] <- 'No'
credit$default[credit$default == 2] <- 'Yes'
credit$default <- factor(credit$default)
str(credit$default)
table(credit$checking_balance)
table(credit$savings_balance)
summary(credit$months_loan_duration)
summary(credit$amount)
table(credit$default)

```

###Data Preparation - Creating Random Training and Test Datasets
```{r }
#generize random number for the training dataset
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)

credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]

#check the randomization
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
```
##Step 3. Training a Model on Data
```{r }
library(C50)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
summary(credit_model)
```
Translation of the first 4 lines of the output:
1. If the checking account balance is greater than 200 DM or is unknown, then we classify it as "not likely to default."

2. Otherwise, if the checking account balance is less than zero DM or between one and 200 DM:
3. And other_debtors are guarantor:
4. And the month loan duration is greater than 36, then we classify it as "likely to default."
On the first line, 412/50 indicates that among the 412 examples classified as not likely to default, 50 were classified incorrectly.

Translation of the confusion matrix:
The model incorrectly classified 135 of the 900 training instances for an error rate of 15 percent. Among the 135 incorrectly classfied cases, 44 no values were incorrectly classified as yes (false positives), while 91 yes values were misclassified as no (false negatives).

##Step 4. Evaluateing the Model Performance
```{r }
credit_pred <- predict(credit_model, credit_test)
library(gmodels)
CrossTable(credit_test$default, credit_pred, 
           prop.chisq = FALSE, 
           prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted   default'))
```
Among the 100 test loan application records, the model correctly predicted that 60 did not default and 14 did default, resulting in an accuracy of (60 + 14) /100 = 74%  and an error rate of (7 + 14) / 100 = 21%. Expectedly, the performance on the unseen tested data is worse than that on the training data. The model also only correctly predicted 14 of the 33 actual loan defaults in the test data. 

##Step 5. Improving Model Performance
```{r }
credit_boost10 <- C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
summary(credit_boost10) #The tree size shrunk
```
We can tell that the new classfier made 3 + 26 = 29 missclassfications out of 900 training samples for an error rate of 3.2%, which is quite an improvement compared to the previous 15%. 
```{r }
credit_boost_pred10 <- predict(credit_boost10, credit_test)

CrossTable(credit_test$default, credit_boost_pred10, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```
Now we see that the total accuracy rate is (60 + 16) / 100 = 76% versus the total error rate of (7 + 17) / 100 = 24%. The model is still not doing well as the 3% increasement in error rate. 

###Making mistakes more costlier than others
```{r}
#construct the cost matrix
matrix_dimensions <- list(c("No", "Yes"), c("No", "Yes"))
names(matrix_dimensions) <- c("predicted", "actual")
matrix_dimensions
```
The values are supplied in specific order
Predicted no, actual no
Predicted yes, actual no
Predicted no, actual yes
Predicted yes, actual yes
```{r}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost
```
There is no cost when the algorithm classifies a no or yes correctly, yet false negative has a cost of 4 versus a false positive's cost of 1. We will see how this impact the classification:
```{r}
credit_cost <- C5.0(credit_train[-17], credit_train$default, costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)
CrossTable(credit_test$default, credit_cost_pred, 
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual default', 'predicted default'))
```
The error rate boosts significantly in this version, which is (34 + 7) / 100 = 41% 

#Problem 2.
##Step 1 & 2 Collecting, Exploring, and Preparing the Data
```{r }
mushrooms <- read.csv("/Users/sunmengyue/Dropbox/2. Northeastern Unviersity/Fall2018/DA 5030 DataMining and Machine Learning/week 7/Assignment 5/mushrooms.csv", stringsAsFactors = TRUE)
str(mushrooms)
#drop the veil_type variable
mushrooms$veil_type <- NULL
table(mushrooms$type)
#4208 of the mushrooms are edible, while 3916 are poisonous.
```

##Step 3. Training the Model on the Data
```{r }
library(RWeka)
mushroom_1R <- OneR(type ~ ., data = mushrooms)#consider all possible features
mushroom_1R
```
The odor feature was selected for rule generation. If the mushroom smells fishy, foul, musty, pungent, spicy, or creosote-like, the mushroom tend to be poisonous. On the other hand, if the mushroom smells like almond and anise, or those with no smell are predicted to be edible.

##Step 4. Evaluating the Model Performance
```{r}
summary(mushroom_1R)
```
The classifier did not classify any edible mushrooms as poisonous, it classify 120 poisonous mushrooms as edible.

##Step 5. Improving Model Performance
```{r}
mushroom_JRip <- JRip(type ~ ., data = mushrooms)
mushroom_JRip
```
The first 3 line can be translated as:
If the odor is foul, then the mushroom type is poisonous.
If the gill size is narrow and the gill color is buff, then the mushroom type is poisonous.
If the gill size is narrow and the odor is pungent, then the mushroom type is poisonous
Mushrooms not covered by the 9 rules are edible.
```{r }
summary(mushroom_JRip)
```
This time we see no mushrooms are misclassified.

#Problem 3
##KNN
*The advantage of KNN include it is easy to understand and implement. It works well on small dataset.
As the KNN assumes that the data are connected in a feature space, then data points belongs to the same category are close in distance. For example, all mammals are closely connected and all fishes are closely linked. 
*The disadvantage of KNN is that when there is a large number of unlabled neighbors, the computational costs increases and the running time is high. Also, the space complexity can be high to calculate large number of unlabled neighbors. 

##Naive Bayes
*The advantage of Naive Bayes is that it is a great choice for text classification. For example, we can use Navive Bayes to predict whether a message is spam or ham.
*The disadvantage of Naive Bayes is that it assumes that the feature are independent. However, if features are correlated with each other, the performance of naive bayes decrease.  

##C5.0 Decision Tree
*The advantage of using decision trees is that the flow chart of data classfication is easy to understand and interpret.
Suppose we are predicting whether we will be late or not for work based on the time we leave home. Leaving before 7:00 will not lead to tardiness, yet leaving after 7:00 will belate. We can further break down the time point at before and after 6:45 and before and after 7:30. 
*However, how decision trees perform highly depend on the features chosen. If highly relevent or interactive features exist, then the performance of the decision tree will decrease.
Given the last example, if there is one more feature named traffic in the model, then there is a relationship between traffic and the time we leave home. If we also take the season into consideration, we will find the traffic is also correlated with the season. More complicated relations increases as relevant features increase such as wether, road construction, etc. 
*Also, the decision trees can be easily overfit.
For instance, if wake up time was plot against age and the age group was devided to narrowly that not only the trend was captured but also the noise in the given data set was put into consideration. The model will be less helpful in new data. 

##RIPPER RULE
*The RIPPER RULE is easy to understand because it follows the logic pattern of "if....then". For instance, if an animal can fly and it has beak and feather, then it is a bird. 
*The diadvantage of ripper rule is that the features are usually categorical. If you have continuous numeric variable, you need to set the feature into groups and transfer them into categorical. For example, if you want to predict the type of trees based on the altitude they grow, you need to group the altitudes first. 